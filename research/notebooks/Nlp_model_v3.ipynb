{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":14033006,"sourceType":"datasetVersion","datasetId":8936158},{"sourceId":284349202,"sourceType":"kernelVersion"}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q transformers datasets scikit-learn\n\nimport pandas as pd\nimport numpy as np\nimport torch\nimport os\nfrom sklearn.model_selection import train_test_split\nfrom transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification, Trainer, TrainingArguments\nfrom datasets import Dataset, load_dataset\n\n# Set Device\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Using device: {device}\")","metadata":{"_uuid":"cdfd6de7-06fd-4b55-a720-76e23afb9bdc","_cell_guid":"dc7691a2-746d-4dd2-bb9c-4d5158b726ca","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-12-28T09:24:11.421440Z","iopub.execute_input":"2025-12-28T09:24:11.421664Z","iopub.status.idle":"2025-12-28T09:24:55.324912Z","shell.execute_reply.started":"2025-12-28T09:24:11.421618Z","shell.execute_reply":"2025-12-28T09:24:55.324094Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==========================================\n# 1. LOAD UNSAFE DATA (Class 1)\n# ==========================================\nprint(\"Loading UNSAFE data (Big Porn Dataset)...\")\n\n# We only load 40,000 rows. This is enough to learn patterns without crashing RAM.\n# We use the special 'â€½' separator you found.\nunsafe_df = pd.read_csv(\n    \"/kaggle/input/big-porn-dataset-fyp/data.csv\", \n    sep='â€½', \n    engine='python', \n    on_bad_lines='skip',\n    nrows=40000 \n)\n\n# Combine Title and Tags into one text string for the model\nunsafe_df['text'] = unsafe_df['title'].fillna(\"\") + \" \" + unsafe_df['tags'].fillna(\"\")\nunsafe_df['label'] = 1  # 1 = UNSAFE\nunsafe_df = unsafe_df[['text', 'label']]\n\nprint(f\"âœ… Loaded {len(unsafe_df)} Unsafe samples.\")\n\n# ==========================================\n# 2. LOAD SAFE DATA (Class 0)\n# ==========================================\nprint(\"Loading SAFE data (WikiText)...\")\n# We use Wikipedia as a proxy for \"Safe/General\" internet content\nsafe_dataset = load_dataset(\"wikitext\", \"wikitext-2-v1\", split=\"train[:40000]\")\n\nsafe_list = [x['text'] for x in safe_dataset if len(x['text']) > 50] # Filter short junk\nsafe_df = pd.DataFrame({'text': safe_list, 'label': 0}) # 0 = SAFE\nsafe_df = safe_df.head(40000) # Match size of Unsafe data\n\nprint(f\"âœ… Loaded {len(safe_df)} Safe samples.\")\n\n# ==========================================\n# 3. LOAD MEDICAL CONTEXT (Class 2)\n# ==========================================\nprint(\"Loading MEDICAL data (AG News)...\")\n# This ensures we don't block biology/health sites\nsci_dataset = load_dataset(\"ag_news\", split=\"train[:50000]\")\n\nmedical_keywords = ['health', 'cancer', 'treatment', 'patient', 'doctor', 'surgery', 'anatomy', 'biology', 'reproductive', 'breast', 'virus']\nmedical_list = []\n\nfor item in sci_dataset:\n    if item['label'] == 3: # 3 is Sci/Tech\n        if any(word in item['text'].lower() for word in medical_keywords):\n            medical_list.append(item['text'])\n\n# Augment (Duplicate) medical data if it's too small, so the model learns it well\nmedical_df = pd.DataFrame({'text': medical_list, 'label': 2}) # 2 = MEDICAL\nif len(medical_df) < 5000:\n    medical_df = pd.concat([medical_df] * 5, ignore_index=True)\nmedical_df = medical_df.head(40000)\n\nprint(f\"âœ… Loaded {len(medical_df)} Medical samples.\")\n\n# ==========================================\n# 4. MERGE EVERYTHING\n# ==========================================\ndf = pd.concat([unsafe_df, safe_df, medical_df], ignore_index=True)\ndf = df.sample(frac=1, random_state=42).reset_index(drop=True) # Shuffle\n\nprint(\"\\nFinal Dataset Distribution:\")\nprint(df['label'].value_counts())","metadata":{"_uuid":"e592fb4a-7366-4bf8-840c-3890813d865a","_cell_guid":"e05df122-59ac-40fa-84f5-0e1275be4ec7","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-12-28T09:24:55.326527Z","iopub.execute_input":"2025-12-28T09:24:55.327055Z","iopub.status.idle":"2025-12-28T09:25:03.662069Z","shell.execute_reply.started":"2025-12-28T09:24:55.327033Z","shell.execute_reply":"2025-12-28T09:25:03.661341Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 1. SPLIT TRAIN / TEST\ntrain_df, val_df = train_test_split(df, test_size=0.15, random_state=42)\n\n# Convert to Hugging Face format\ntrain_dataset = Dataset.from_pandas(train_df)\nval_dataset = Dataset.from_pandas(val_df)\n\n# 2. TOKENIZER\nmodel_name = \"distilbert-base-uncased\"\ntokenizer = DistilBertTokenizerFast.from_pretrained(model_name)\n\ndef tokenize(batch):\n    return tokenizer(batch[\"text\"], padding=\"max_length\", truncation=True, max_length=128)\n\nprint(\"Tokenizing... (This takes 1-2 mins)\")\ntrain_tokenized = train_dataset.map(tokenize, batched=True)\nval_tokenized = val_dataset.map(tokenize, batched=True)\n\n# 3. LOAD MODEL\nmodel = DistilBertForSequenceClassification.from_pretrained(\n    model_name,\n    num_labels=3,\n    id2label={0: \"SAFE\", 1: \"UNSAFE\", 2: \"MEDICAL\"},\n    label2id={\"SAFE\": 0, \"UNSAFE\": 1, \"MEDICAL\": 2}\n).to(device)\n\n# 4. TRAINING CONFIG (Corrected for New Version)\ntraining_args = TrainingArguments(\n    output_dir=\"./fyp_model_output\",\n    num_train_epochs=2,              \n    per_device_train_batch_size=32,  \n    per_device_eval_batch_size=64,\n    eval_strategy=\"epoch\",           # <--- CHANGED FROM evaluation_strategy\n    save_strategy=\"epoch\",\n    learning_rate=2e-5,\n    weight_decay=0.01,\n    load_best_model_at_end=True,\n    report_to=\"none\"\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_tokenized,\n    eval_dataset=val_tokenized,\n)\n\n# 5. RUN TRAINING\nprint(\"ðŸš€ Starting Training...\")\ntrainer.train()","metadata":{"_uuid":"d4618d35-7d8c-4d56-bff6-e4e878cc82f6","_cell_guid":"ed323383-7175-42bd-be3d-96a17a0945c1","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-12-28T09:25:03.662808Z","iopub.execute_input":"2025-12-28T09:25:03.663066Z","iopub.status.idle":"2025-12-28T09:36:17.325138Z","shell.execute_reply.started":"2025-12-28T09:25:03.663040Z","shell.execute_reply":"2025-12-28T09:36:17.324558Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 1. SAVE TO DISK\nsave_path = \"fyp_final_model\"\nmodel.save_pretrained(save_path)\ntokenizer.save_pretrained(save_path)\nprint(f\"Model saved to {save_path}\")\n\n# 2. TEST IT IMMEDIATELY\ndef predict(text):\n    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=128).to(device)\n    with torch.no_grad():\n        logits = model(**inputs).logits\n    pred_id = logits.argmax().item()\n    return model.config.id2label[pred_id]\n\nprint(\"\\n--- TEST RESULTS ---\")\nprint(\"Test 1 (Safe):\", predict(\"The python programming language is great.\"))\nprint(\"Test 2 (Porn):\", predict(\"Hardcore xxx video free download.\"))\nprint(\"Test 3 (Medical):\", predict(\"The anatomy of the reproductive system.\"))\n\n# 3. ZIP FOR DOWNLOAD\n!zip -r fyp_final_model.zip fyp_final_model\nprint(\"\\nâœ… DONE! Download 'fyp_final_model.zip' from the Output tab on the right.\")","metadata":{"_uuid":"5b9c4a5c-69fd-4077-aba4-70d53283777e","_cell_guid":"41c989d8-9b43-4630-8646-6a291a557de8","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-12-28T09:37:59.952529Z","iopub.execute_input":"2025-12-28T09:37:59.952817Z","iopub.status.idle":"2025-12-28T09:38:14.660137Z","shell.execute_reply.started":"2025-12-28T09:37:59.952798Z","shell.execute_reply":"2025-12-28T09:38:14.659365Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"### Rigirous testiing\nimport pandas as pd\nimport torch\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification\nfrom datasets import load_dataset\nfrom tqdm import tqdm\n\n# ==========================================\n# 1. SETUP\n# ==========================================\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nMODEL_PATH = \"fyp_final_model\" # Folder where you saved the model\n\nprint(f\"Loading model from {MODEL_PATH} on {device}...\")\ntry:\n    tokenizer = DistilBertTokenizerFast.from_pretrained(MODEL_PATH)\n    model = DistilBertForSequenceClassification.from_pretrained(MODEL_PATH).to(device)\nexcept OSError:\n    print(\"âŒ Model not found! Did you run the training cell?\")\n    exit()\n\n# ==========================================\n# 2. CREATE \"HARD MODE\" TEST DATASET\n# ==========================================\nprint(\"\\nBUILDING RIGOROUS TEST SET...\")\n\n# A. UNSEEN PORN (Skip the first 50k rows used for training)\nprint(\"1. Loading Unseen Porn (Rows 50k-51k)...\")\ndf_porn = pd.read_csv(\n    \"/kaggle/input/big-porn-dataset-fyp/data.csv\", \n    sep='â€½', engine='python', on_bad_lines='skip',\n    skiprows=50000, nrows=500  # <--- SKIPPING TRAINING DATA\n)\ndf_porn['text'] = df_porn.iloc[:, 2].fillna(\"\") + \" \" + df_porn.iloc[:, 3].fillna(\"\") # Title + Tags\ndf_porn['true_label'] = 1 # UNSAFE\ndf_porn = df_porn[['text', 'true_label']]\n\n# B. UNSEEN MEDICAL (PubMed Abstracts - Highly Technical)\nprint(\"2. Loading Unseen Medical (PubMed)...\")\nds_med = load_dataset(\"pubmed_qa\", \"pqa_labeled\", split=\"train[:500]\")\ndf_med = pd.DataFrame({'text': ds_med['question'], 'true_label': 2}) # 2 = MEDICAL\n\n# C. UNSEEN SAFE (IMDB Movie Reviews - Casual/Slang)\nprint(\"3. Loading Unseen Safe (IMDB Reviews)...\")\nds_safe = load_dataset(\"imdb\", split=\"test[:500]\")\ndf_safe = pd.DataFrame({'text': ds_safe['text'], 'true_label': 0}) # 0 = SAFE\n\n# Combine and Shuffle\ntest_df = pd.concat([df_porn, df_med, df_safe], ignore_index=True)\ntest_df = test_df.sample(frac=1, random_state=42).reset_index(drop=True)\n\nprint(f\"âœ… Created Test Set with {len(test_df)} distinct samples.\")\n\n# ==========================================\n# 3. EVALUATION LOOP\n# ==========================================\nprint(\"\\nRunning Evaluation (This takes ~30 seconds)...\")\n\npredictions = []\ntrue_labels = test_df['true_label'].tolist()\n\n# Run in batches to be fast\nbatch_size = 32\ntexts = test_df['text'].tolist()\n\nmodel.eval() # Set model to evaluation mode\n\nfor i in tqdm(range(0, len(texts), batch_size)):\n    batch_texts = texts[i : i + batch_size]\n    \n    # Tokenize\n    inputs = tokenizer(\n        batch_texts, \n        return_tensors=\"pt\", \n        truncation=True, \n        padding=True, \n        max_length=128\n    ).to(device)\n    \n    # Predict\n    with torch.no_grad():\n        logits = model(**inputs).logits\n    \n    # Store predictions\n    preds = torch.argmax(logits, dim=1).cpu().numpy()\n    predictions.extend(preds)\n\n# ==========================================\n# 4. METRICS & VISUALIZATION\n# ==========================================\ntarget_names = [\"SAFE\", \"UNSAFE (Porn)\", \"MEDICAL\"]\n\nprint(\"\\n\" + \"=\"*30)\nprint(\"   CLASSIFICATION REPORT\")\nprint(\"=\"*30)\nprint(classification_report(true_labels, predictions, target_names=target_names))\n\n# Plot Confusion Matrix\ncm = confusion_matrix(true_labels, predictions)\nplt.figure(figsize=(8, 6))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=target_names, yticklabels=target_names)\nplt.xlabel('Predicted by Model')\nplt.ylabel('Actual Label')\nplt.title('Where did the model get confused?')\nplt.show()","metadata":{"_uuid":"0cc17dd9-4e3f-4112-9de5-874614f693e9","_cell_guid":"e89ca087-f287-4f3f-a82f-8e4fb0e9be9c","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-12-28T09:38:14.661505Z","iopub.execute_input":"2025-12-28T09:38:14.662190Z","iopub.status.idle":"2025-12-28T09:38:21.943871Z","shell.execute_reply.started":"2025-12-28T09:38:14.662158Z","shell.execute_reply":"2025-12-28T09:38:21.943294Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"_uuid":"96f718c5-2b05-4ca4-80fb-1daec6e0df68","_cell_guid":"e3b6f349-1e7c-4150-953b-1db573f81d11","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==========================================\n# REPAIR & FINE-TUNE SCRIPT (WITH .PTH EXPORT)\n# ==========================================\nimport os\nimport torch\nfrom datasets import load_dataset, Dataset\nimport pandas as pd\nfrom transformers import Trainer, TrainingArguments\n\n# 1. DISABLE WANDB\nos.environ[\"WANDB_DISABLED\"] = \"true\"\n\nprint(\"Loading conversational data for fine-tuning...\")\n\n# Unsafe (Comments)\nds_unsafe = load_dataset(\"google/civil_comments\", split=\"train[:20000]\")\nunsafe_list = [x['text'] for x in ds_unsafe if x['sexual_explicit'] > 0.5][:1000]\ndf_unsafe = pd.DataFrame({'text': unsafe_list, 'label': 1})\n\n# Safe (News)\nds_safe = load_dataset(\"cnn_dailymail\", \"3.0.0\", split=\"train[:1000]\")\nsafe_list = [x['article'][:500] for x in ds_safe]\ndf_safe = pd.DataFrame({'text': safe_list, 'label': 0})\n\n# Medical (Drugs)\nds_med = load_dataset(\"ade_corpus_v2\", \"Ade_corpus_v2_classification\", split=\"train[:1000]\")\nmed_list = [x['text'] for x in ds_med]\ndf_med = pd.DataFrame({'text': med_list, 'label': 2})\n\n# Merge dataset\nrepair_df = pd.concat([df_unsafe, df_safe, df_med], ignore_index=True)\nrepair_df = repair_df.sample(frac=1, random_state=42).reset_index(drop=True)\n\n# Convert to HF Dataset\nrepair_dataset = Dataset.from_pandas(repair_df)\n\n# Tokenize\nrepair_tokenized = repair_dataset.map(\n    lambda x: tokenizer(\n        x[\"text\"], \n        padding=\"max_length\",\n        truncation=True,\n        max_length=128\n    ), \n    batched=True\n)\n\nprint(f\"Fine-tuning on {len(repair_df)} new conversational samples...\")\n\n# 3. TRAINER\ntraining_args = TrainingArguments(\n    output_dir=\"./fyp_repaired_model\",\n    num_train_epochs=2,\n    per_device_train_batch_size=16,\n    learning_rate=1e-5,\n    save_strategy=\"no\",\n    report_to=\"none\",\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=repair_tokenized,\n)\n\nprint(\"Starting Fine-Tuning...\")\ntrainer.train()\n\n# 4. SAVE HUGGINGFACE MODEL\nsave_dir = \"fyp_final_model_v2\"\nmodel.save_pretrained(save_dir)\ntokenizer.save_pretrained(save_dir)\n\n# 5. SAVE AS .PTH CHECKPOINT\npth_path = \"fyp_final_model_v2/model.pth\"\ntorch.save(model.state_dict(), pth_path)\n\nprint(\"Saved HF model at:\", save_dir)\nprint(\"Saved PyTorch .pth file at:\", pth_path)\nprint(\"Done.\")","metadata":{"_uuid":"b1decee6-fb5f-4024-a518-20a92bfaef71","_cell_guid":"338f7d9c-e75f-4c92-8e7c-eeba9dcd7f8b","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-12-28T09:36:45.450661Z","iopub.execute_input":"2025-12-28T09:36:45.450860Z","iopub.status.idle":"2025-12-28T09:37:40.510834Z","shell.execute_reply.started":"2025-12-28T09:36:45.450845Z","shell.execute_reply":"2025-12-28T09:37:40.510127Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport pandas as pd\nfrom datasets import load_dataset\nfrom transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification\nfrom sklearn.metrics import classification_report\nfrom tqdm import tqdm\n\n# ==========================================\n# 1. SETUP - POINT TO THE NEW v2 MODEL\n# ==========================================\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nMODEL_PATH = \"fyp_final_model_v2\"  # <--- USING THE REPAIRED BRAIN\n\nprint(f\"Loading Improved Model from {MODEL_PATH}...\")\ntry:\n    tokenizer = DistilBertTokenizerFast.from_pretrained(MODEL_PATH)\n    model = DistilBertForSequenceClassification.from_pretrained(MODEL_PATH).to(device)\nexcept OSError:\n    print(\"âŒ Error: Model not found. Did the fine-tuning finish?\")\n    exit()\n\n# ==========================================\n# 2. PREPARE TEST DATA (Unseen Samples)\n# ==========================================\nprint(\"\\nLoading test data...\")\n\n# A. UNSAFE (Comments) - We test on rows 50,000+ (Unseen)\nds_unsafe = load_dataset(\"google/civil_comments\", split=\"train[50000:52000]\") # Different chunk than training\nunsafe_list = [x['text'] for x in ds_unsafe if x['sexual_explicit'] > 0.5][:300]\ndf_unsafe = pd.DataFrame({'text': unsafe_list, 'true_label': 1})\n\n# B. SAFE (News) - Test split\nds_safe = load_dataset(\"cnn_dailymail\", \"3.0.0\", split=\"test[:300]\")\nsafe_list = [x['article'][:500] for x in ds_safe]\ndf_safe = pd.DataFrame({'text': safe_list, 'true_label': 0})\n\n# C. MEDICAL (Drugs) - Test split (if available) or end of train\nds_med = load_dataset(\"ade_corpus_v2\", \"Ade_corpus_v2_classification\", split=\"train[-300:]\") # Last 300\nmed_list = [x['text'] for x in ds_med]\ndf_med = pd.DataFrame({'text': med_list, 'true_label': 2})\n\n# Combine\ntest_df = pd.concat([df_unsafe, df_safe, df_med], ignore_index=True)\ntest_df = test_df.sample(frac=1, random_state=42).reset_index(drop=True)\nprint(f\"âœ… Ready to test on {len(test_df)} samples.\")\n\n# ==========================================\n# 3. RUN PREDICTION\n# ==========================================\npredictions = []\ntrue_labels = test_df['true_label'].tolist()\ntexts = test_df['text'].tolist()\n\nmodel.eval()\nbatch_size = 32\n\nfor i in tqdm(range(0, len(texts), batch_size)):\n    batch_texts = texts[i : i + batch_size]\n    inputs = tokenizer(batch_texts, return_tensors=\"pt\", truncation=True, padding=True, max_length=128).to(device)\n    with torch.no_grad():\n        logits = model(**inputs).logits\n    predictions.extend(torch.argmax(logits, dim=1).cpu().numpy())\n\n# ==========================================\n# 4. RESULTS\n# ==========================================\ntarget_names = [\"SAFE (News)\", \"UNSAFE (Comments)\", \"MEDICAL (Drugs)\"]\nprint(\"\\n\" + \"=\"*40)\nprint(\"   IMPROVED MODEL RESULTS (v2)\")\nprint(\"=\"*40)\nprint(classification_report(true_labels, predictions, target_names=target_names))","metadata":{"_uuid":"08d2aab0-dc1f-491e-97b7-8e85cfa41db7","_cell_guid":"6036023f-565a-4989-b0e3-6f6a0f5413c0","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-12-28T09:37:40.512559Z","iopub.execute_input":"2025-12-28T09:37:40.512785Z","iopub.status.idle":"2025-12-28T09:37:45.083503Z","shell.execute_reply.started":"2025-12-28T09:37:40.512766Z","shell.execute_reply":"2025-12-28T09:37:45.082600Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report\n\n# Class labels\nclass_names = [\"SAFE (News)\", \"UNSAFE (Comments)\", \"MEDICAL (Drugs)\"]\nlabels = [0, 1, 2]\n\n# ===============================\n# 1. CONFUSION MATRIX\n# ===============================\ncm = confusion_matrix(true_labels, predictions, labels=labels)\n\ndisp = ConfusionMatrixDisplay(\n    confusion_matrix=cm,\n    display_labels=class_names\n)\n\nplt.figure()\ndisp.plot(cmap=None)\nplt.title(\"Confusion Matrix for Content Classification Model v2\")\nplt.tight_layout()\nplt.show()\n\n# ===============================\n# 2. F1 SCORE BAR GRAPH\n# ===============================\nreport = classification_report(\n    true_labels,\n    predictions,\n    target_names=class_names,\n    output_dict=True\n)\n\nf1_scores = [\n    report[\"SAFE (News)\"][\"f1-score\"],\n    report[\"UNSAFE (Comments)\"][\"f1-score\"],\n    report[\"MEDICAL (Drugs)\"][\"f1-score\"]\n]\n\nplt.figure()\nplt.bar(class_names, f1_scores)\nplt.ylim(0, 1.05)\nplt.ylabel(\"F1 Score\")\nplt.title(\"F1 Score per Class for Model v2\")\n\nfor i, score in enumerate(f1_scores):\n    plt.text(i, score + 0.01, f\"{score:.2f}\", ha=\"center\")\n\nplt.tight_layout()\nplt.show()","metadata":{"_uuid":"64a84343-67f2-4a82-adf5-b283d9e06469","_cell_guid":"0b075d3b-bc15-4f68-8cde-2ad67c06b7e2","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-12-28T09:40:27.296407Z","iopub.execute_input":"2025-12-28T09:40:27.296732Z","iopub.status.idle":"2025-12-28T09:40:27.653357Z","shell.execute_reply.started":"2025-12-28T09:40:27.296708Z","shell.execute_reply":"2025-12-28T09:40:27.652736Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom datasets import load_dataset\nfrom transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom tqdm import tqdm\n\n# ==========================================\n# 1. SETUP\n# ==========================================\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nMODEL_PATH = \"fyp_final_model_v2\" # Using the REPAIRED model\n\nprint(f\"Loading Fine-Tuned Brain from {MODEL_PATH}...\")\ntry:\n    tokenizer = DistilBertTokenizerFast.from_pretrained(MODEL_PATH)\n    model = DistilBertForSequenceClassification.from_pretrained(MODEL_PATH).to(device)\nexcept OSError:\n    print(\"âŒ Error: Model not found. Please run the Fine-Tuning cell first!\")\n    exit()\n\n# ==========================================\n# 2. LOAD 3 NEW DATASETS (The Rigorous Test)\n# ==========================================\nprint(\"\\nðŸ“¥ Downloading 3 NEW datasets for Stress Testing...\")\n\n# --- A. UNSAFE TEST (Offensive Tweets) ---\n# Dataset: tweet_eval (offensive)\n# Challenge: Slang, typos, short text.\nprint(\"1. Loading 'TweetEval' (Offensive Tweets)...\")\nds_unsafe = load_dataset(\"tweet_eval\", \"offensive\", split=\"test\")\n# Filter for label 1 (Offensive) and take 300\nunsafe_list = [x['text'] for x in ds_unsafe if x['label'] == 1][:300]\ndf_unsafe = pd.DataFrame({'text': unsafe_list, 'true_label': 1}) # 1 = UNSAFE\nprint(f\"   -> Found {len(df_unsafe)} offensive tweets.\")\n\n# --- B. SAFE TEST (General Knowledge) ---\n# Dataset: dbpedia_14\n# Challenge: Encyclopedic text about Companies/Artists (Must be allowed)\nprint(\"2. Loading 'DBPedia' (General Knowledge)...\")\nds_safe = load_dataset(\"dbpedia_14\", split=\"test[:300]\")\nsafe_list = [x['content'][:200] for x in ds_safe] # First 200 chars\ndf_safe = pd.DataFrame({'text': safe_list, 'true_label': 0}) # 0 = SAFE\nprint(f\"   -> Found {len(df_safe)} encyclopedia entries.\")\n\n# --- C. MEDICAL TEST (Exam Questions) ---\n# Dataset: medmcqa\n# Challenge: Very complex biology/anatomy questions.\nprint(\"3. Loading 'MedMCQA' (Medical Exams)...\")\nds_med = load_dataset(\"medmcqa\", split=\"test[:300]\")\nmed_list = [x['question'] for x in ds_med]\ndf_med = pd.DataFrame({'text': med_list, 'true_label': 2}) # 2 = MEDICAL\nprint(f\"   -> Found {len(df_med)} medical questions.\")\n\n# Combine\ntest_df = pd.concat([df_unsafe, df_safe, df_med], ignore_index=True)\ntest_df = test_df.sample(frac=1, random_state=42).reset_index(drop=True)\n\nprint(f\"\\nâœ… Ready to test on {len(test_df)} completely new samples.\")\n\n# ==========================================\n# 3. RUN PREDICTIONS\n# ==========================================\npredictions = []\ntrue_labels = test_df['true_label'].tolist()\ntexts = test_df['text'].tolist()\n\nbatch_size = 32\nmodel.eval()\n\nprint(\"Running AI analysis...\")\nfor i in tqdm(range(0, len(texts), batch_size)):\n    batch_texts = texts[i : i + batch_size]\n    \n    inputs = tokenizer(\n        batch_texts, return_tensors=\"pt\", truncation=True, padding=True, max_length=128\n    ).to(device)\n    \n    with torch.no_grad():\n        logits = model(**inputs).logits\n    \n    preds = torch.argmax(logits, dim=1).cpu().numpy()\n    predictions.extend(preds)\n\n# ==========================================\n# 4. FINAL REPORT & VISUALIZATION\n# ==========================================\ntarget_names = [\"SAFE (DBPedia)\", \"UNSAFE (Tweets)\", \"MEDICAL (Exams)\"]\n\nprint(\"\\n\" + \"=\"*40)\nprint(\"   FINAL RIGOROUS EVALUATION REPORT\")\nprint(\"=\"*40)\nprint(classification_report(true_labels, predictions, target_names=target_names))\n\n# Create Confusion Matrix\ncm = confusion_matrix(true_labels, predictions)\nplt.figure(figsize=(8, 6))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Greens', xticklabels=target_names, yticklabels=target_names)\nplt.xlabel('AI Predicted')\nplt.ylabel('Actual Category')\nplt.title('Final Model Robustness Check')\nplt.show()\n\n# Show Mistakes (If any)\nprint(\"\\n--- ANALYZING MISTAKES ---\")\nmistakes = 0\nfor i in range(len(texts)):\n    if predictions[i] != true_labels[i]:\n        print(f\"Mistake: AI said '{target_names[predictions[i]]}' | Real is '{target_names[true_labels[i]]}'\")\n        print(f\"Text: {texts[i][:100]}...\\n\")\n        mistakes += 1\n        if mistakes > 5: break","metadata":{"_uuid":"d64a291b-6e5e-4d71-b501-80bcf063536b","_cell_guid":"d4648dc9-f6f9-4a1d-920d-4e9a050fbe07","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-12-28T09:37:45.084474Z","iopub.execute_input":"2025-12-28T09:37:45.084725Z","iopub.status.idle":"2025-12-28T09:37:59.947308Z","shell.execute_reply.started":"2025-12-28T09:37:45.084704Z","shell.execute_reply":"2025-12-28T09:37:59.946661Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import label_binarize\nfrom sklearn.metrics import roc_curve, auc\n\n# ===============================\n# 1. GET PROBABILITIES\n# ===============================\nmodel.eval()\nall_probs = []\n\nbatch_size = 32\nfor i in range(0, len(texts), batch_size):\n    batch_texts = texts[i : i + batch_size]\n    inputs = tokenizer(\n        batch_texts,\n        return_tensors=\"pt\",\n        truncation=True,\n        padding=True,\n        max_length=128\n    ).to(device)\n\n    with torch.no_grad():\n        logits = model(**inputs).logits\n        probs = torch.softmax(logits, dim=1)\n\n    all_probs.append(probs.cpu().numpy())\n\ny_score = np.vstack(all_probs)\ny_true = np.array(true_labels)\n\n# ===============================\n# 2. BINARIZE LABELS\n# ===============================\nclass_names = [\"SAFE (News)\", \"UNSAFE (Comments)\", \"MEDICAL (Drugs)\"]\ny_true_bin = label_binarize(y_true, classes=[0, 1, 2])\n\n# ===============================\n# 3. ROC CURVES\n# ===============================\nplt.figure()\n\nfor i, class_name in enumerate(class_names):\n    fpr, tpr, _ = roc_curve(y_true_bin[:, i], y_score[:, i])\n    roc_auc = auc(fpr, tpr)\n    plt.plot(fpr, tpr, label=f\"{class_name} (AUC = {roc_auc:.2f})\")\n\nplt.plot([0, 1], [0, 1], linestyle=\"dotted\")\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.title(\"ROC Curves for Content Classification Model v2\")\nplt.legend()\nplt.tight_layout()\nplt.show()","metadata":{"_uuid":"015d835d-13f6-4355-9bee-276580b97f59","_cell_guid":"ebcb9f3c-ee5b-4340-83ce-9a056e643256","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-12-28T09:42:21.092669Z","iopub.execute_input":"2025-12-28T09:42:21.093234Z","iopub.status.idle":"2025-12-28T09:42:26.122437Z","shell.execute_reply.started":"2025-12-28T09:42:21.093209Z","shell.execute_reply":"2025-12-28T09:42:26.121814Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"_uuid":"e2d146e5-0a7d-4257-b4de-848353a2e2f2","_cell_guid":"ba0296df-c5bc-407d-992c-16b00165fe85","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"_uuid":"f0f3950e-ea7d-4ed3-a677-9d8fb039087c","_cell_guid":"038dcb4c-152f-4f75-b58f-ee49d99e5042","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null}]}